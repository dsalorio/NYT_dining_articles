{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of all recipes on the New York Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://cooking.nytimes.com/search'\n",
    "recipe_urls = []\n",
    "image_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "response = requests.get(base_url)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe urls\n",
    "def get_recipe_and_image_urls(url):\n",
    "    '''Given a URL in the NYT's recipe search system, make a tuple containing the recipe url and image url of each recipe \n",
    "    on that page, and return all the tuples for that page as a list'''\n",
    "    \n",
    "    recipes_and_images=[]\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "    for article in soup.find_all('article',attrs={\"class\":\"card recipe-card\"}):\n",
    "        recipe_url= 'https://cooking.nytimes.com' + article['data-url']\n",
    "        \n",
    "        if '.jpg' in article['data-seo-image-url']:\n",
    "            image_url = article['data-seo-image-url']\n",
    "        else:\n",
    "            image_url = None\n",
    "        \n",
    "        recipes_and_images.append((recipe_url, image_url))\n",
    "        \n",
    "    return recipes_and_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = []\n",
    "\n",
    "for num in range(1,417): #416 pages of recipes in search- this will only go up with time\n",
    "    time.sleep(.5+2*random.random())\n",
    "    search_url = base_url + '?q=&page=' + str(num)\n",
    "    recipes.extend(get_recipe_and_image_urls(search_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19933\n",
      "[('https://cooking.nytimes.com/recipes/1016999-rhubarb-big-crumb-coffeecake', 'https://static01.nyt.com/images/2015/03/16/dining/big-crumb-coffee-cake/big-crumb-coffee-cake-superJumbo.jpg'), ('https://cooking.nytimes.com/recipes/1015959-myra-waldos-swedish-lamb', 'https://static01.nyt.com/images/2014/01/15/dining/recipes-myrawaldoswedishlamb/recipes-myrawaldoswedishlamb-superJumbo.jpg'), ('https://cooking.nytimes.com/recipes/1017332-salty-dog', 'https://static01.nyt.com/images/2015/04/03/dining/salty-dog/salty-dog-superJumbo-v2.jpg')]\n"
     ]
    }
   ],
   "source": [
    "print(len(recipes))\n",
    "print(recipes[19930:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the list of urls\n",
    "with open('urls_recipe_image.pickle', 'wb') as write_file:\n",
    "    pickle.dump(recipes, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Get recipe descriptions and data, as well as the URLs of the full articles that link to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create db\n",
    "recipe_db = client['recipes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a collection in the db for recipes and their descriptions\n",
    "descr_col = recipe_db['descriptions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cooking.nytimes.com/recipes/8478-candied-squash-and-ginger-relish\n",
      "https://cooking.nytimes.com/recipes/8476-roasted-squash-puree\n",
      "https://cooking.nytimes.com/recipes/8321-monterey-county-jail-oatmeal\n",
      "https://cooking.nytimes.com/recipes/10625-oatmeal-cookies\n",
      "https://cooking.nytimes.com/recipes/9689-green-glory-juice\n",
      "https://cooking.nytimes.com/recipes/1372-hummus\n",
      "https://cooking.nytimes.com/recipes/2461-oatmeal-raisin-cookies\n"
     ]
    }
   ],
   "source": [
    "# add recipe documents to the collection, and track which recipe websites don't work\n",
    "\n",
    "no_website_list = []\n",
    "\n",
    "for recipe in recipes:\n",
    "    time.sleep(.5+2*random.random())\n",
    "    response = requests.get(recipe[0])\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        recipe_dict={\n",
    "            'recipe_url':recipe[0], #recipe url\n",
    "            'image_url':recipe[1] #image url\n",
    "        }\n",
    "        \n",
    "        # article\n",
    "        try:\n",
    "            recipe_dict['article_url'] = soup.find('p',attrs={\"class\":\"related-article\"}).find('a')['href']\n",
    "        except:\n",
    "            pass\n",
    "        # recipe title\n",
    "        try:\n",
    "            recipe_dict['recipe_title'] = soup.find('div',attrs={\"class\":\"title-container\"}).find('h1')['data-name']\n",
    "        except:\n",
    "            pass\n",
    "        #recipe description\n",
    "        try:\n",
    "            recipe_dict['recipe_description'] = soup.find(itemprop='description').find('p').text\n",
    "        except:\n",
    "            pass\n",
    "        #recipe author\n",
    "        try:\n",
    "            recipe_dict['recipe_author'] = soup.find('div',attrs={\"class\":\"recipe-subhead\"}).find('h3').find('a')['data-author']\n",
    "        except:\n",
    "            pass\n",
    "        #recipe date\n",
    "        try:\n",
    "            dt_list = re.findall(r'\\b\\d{4}/\\d\\d?/\\d\\d?\\b', doc['article_url'])\n",
    "            doc['recipe_date'] = datetime.strptime(dt_list[0], '%Y/%m/%d')\n",
    "        except:\n",
    "            try:\n",
    "                dt_list = re.findall(r'\\b\\d{4}/\\d\\d?/\\d\\d?\\b', doc['image_url'])\n",
    "                doc['recipe_date'] = datetime.strptime(dt_list[0], '%Y/%m/%d')\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # add the dictionary to the database\n",
    "        descr_col.insert_one(recipe_dict)\n",
    "        \n",
    "    # add the recipe url to a list of broken links if it doesn't work, and print the recipe url\n",
    "    else:\n",
    "        no_website_list.append(recipe[0])\n",
    "        print(recipe[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a separate collection for the articles that link to recipes, with their associated information, and a list of all recipes that link to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of recipes: 19926\n",
      "number of recipes with an article: 18868\n",
      "number of unique articles: 8215\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of recipes: {len(list(descr_col.find()))}\") \n",
    "print(f\"number of recipes with an article: {len(list(descr_col.find({'article_url': {'$exists':True}})))}\")\n",
    "print(f\"number of unique articles: {len(set(art['article_url'] for art in descr_col.find({'article_url': {'$exists':True}}, {'article_url':1})))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a collection in the db for articles\n",
    "article_col = recipe_db['articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor that we'll use to loop through all the recipes that link to an article\n",
    "recipe_with_article_cursor = descr_col.find({'article_url': {'$exists':True}})\n",
    "\n",
    "for recipe_doc in recipe_with_article_cursor:\n",
    "    \n",
    "    article_url_from_recipe = recipe_doc['article_url']\n",
    "    query = {'article_url': article_url_from_recipe}\n",
    "    \n",
    "    article_doc = article_col.find_one(query)\n",
    "    \n",
    "    recipe_url = recipe_doc['recipe_url']\n",
    "    \n",
    "    if article_doc:\n",
    "        new_recipe_list = article_doc['linked_recipes']\n",
    "        new_recipe_list.append(recipe_url)\n",
    "        new_values = { \"$set\": { 'linked_recipes': new_recipe_list} }\n",
    "        article_col.update_one(query, new_values)\n",
    "    \n",
    "    else:\n",
    "        recipe_list = [recipe_url]\n",
    "        article_dict = {\n",
    "            'article_url':article_url_from_recipe,\n",
    "            'linked_recipes':recipe_list\n",
    "        }\n",
    "        article_col.insert_one(article_dict)\n",
    "\n",
    "recipe_with_article_cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_article(article_soup):\n",
    "    view_str = '(View this recipe in NYT Cooking.)'\n",
    "    save_str = 'Save these essentials to your NYT Cooking recipe box.'\n",
    "    ten_str = '10 Essential Recipes is a new occasional feature that explores different cuisines.'\n",
    "\n",
    "    article_body=''\n",
    "\n",
    "    for div in article_soup.find_all('div',attrs={\"class\":\"StoryBodyCompanionColumn\"})[:-1]:\n",
    "        for next_div in div.find_all('div'):\n",
    "            if next_div.find('p'):\n",
    "                for p in next_div.find_all('p'):\n",
    "                    article_body+=' ' + p.text\n",
    "    \n",
    "    cleaned_article = ' '.join(article_body.replace(view_str, '').replace(save_str, '').replace(ten_str, '').split())\n",
    "    \n",
    "    if cleaned_article == '':\n",
    "        raise Exception\n",
    "    \n",
    "    return cleaned_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_cursor = article_col.find()\n",
    "\n",
    "for article_doc in article_cursor:\n",
    "    time.sleep(.5+2*random.random())\n",
    "    \n",
    "    article_url = article_doc['article_url']\n",
    "    doc_id = article_doc['_id']\n",
    "    response = requests.get(article_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    # article body\n",
    "    try:\n",
    "        article_doc['article_body'] = get_cleaned_article(soup)\n",
    "    except:\n",
    "        pass\n",
    "    # article author\n",
    "    try:\n",
    "        article_doc['article_author'] = soup.find('p',attrs={\"itemprop\":\"author\"}).find('span',attrs={\"itemprop\":\"name\"}).text\n",
    "    except:\n",
    "        # check to make sure there's actually an article body before assuming that it doesn't name an author     \n",
    "        if 'article_body' in article_doc.keys():\n",
    "            article_doc['article_author'] = 'unnamed'\n",
    "    # article date\n",
    "    try:\n",
    "        dt_list = re.findall(r'\\b\\d{4}/\\d\\d?/\\d\\d?\\b', article_url)\n",
    "        article_doc['article_date'] = datetime.strptime(dt_list[0], '%Y/%m/%d')\n",
    "    except:\n",
    "        pass\n",
    "    # article title\n",
    "    try:\n",
    "        article_doc['article_title'] = soup.find('h1', attrs={'itemprop':'headline'}).text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    query = {'_id': doc_id}\n",
    "    new_values = {\"$set\": article_doc}\n",
    "    article_col.update_one(query, new_values)\n",
    "    \n",
    "article_cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
