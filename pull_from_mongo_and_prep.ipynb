{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import re\n",
    "import string\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# stem & lemmatize\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull from Mongo and into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['descriptions']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_db = client.recipes\n",
    "rec_db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr_col = rec_db.descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recipe_url': 'https://cooking.nytimes.com/recipes/1020205-ash-reshteh-persian-greens-bean-and-noodle-soup',\n",
       " 'image_url': 'https://static01.nyt.com/images/2019/05/15/dining/14Iranianrex2/merlin_154113918_721ff786-e3ef-453f-b3a0-deec1d8f8e02-threeByTwoMediumAt2X.jpg',\n",
       " 'recipe_title': 'Ash Reshteh (Persian Greens, Bean and Noodle Soup)',\n",
       " 'recipe_description': 'Ash reshteh’s flavor is defined by two uniquely Persian ingredients: reshteh and kashk. The soup, served during the festivities leading up to Nowruz, the Persian New Year, wouldn’t be the same without the soup noodles called reshteh, which are saltier and starchier than Italian noodles — though you could substitute linguine in a pinch. Kashk, a form of drained yogurt or whey, is saltier and more sour than Greek yogurt or sour cream. More like feta than yogurt, liquid kashk gives ash its distinct, satisfying flavor. If you can’t find liquid kashk, buy it powdered and hydrate it with warm water to the consistency of sour cream. Look for both items at a Middle Eastern grocery.',\n",
       " 'recipe_author': 'Samin Nosrat',\n",
       " 'article_url': 'https://www.nytimes.com/2019/05/14/dining/persian-food-recipes-samin-nosrat.html',\n",
       " 'pub_date': datetime.datetime(2019, 5, 14, 0, 0)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descr_col.find_one({},{ '_id': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select if has a recipe description and a publication date\n",
    "descr_df = pd.DataFrame(list(descr_col.find({'recipe_description': {'$ne':None}, 'pub_date': {'$ne':None}},\n",
    "                                            {'_id':0,'pub_date':1, 'recipe_author':1, 'recipe_description':1, 'recipe_title':1, 'image_url':1, 'recipe_url':1})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any duplicate rows\n",
    "descr_df=descr_df.drop_duplicates('recipe_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>recipe_author</th>\n",
       "      <th>recipe_description</th>\n",
       "      <th>recipe_title</th>\n",
       "      <th>recipe_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://static01.nyt.com/images/2019/05/15/din...</td>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>Samin Nosrat</td>\n",
       "      <td>Ash reshteh’s flavor is defined by two uniquel...</td>\n",
       "      <td>Ash Reshteh (Persian Greens, Bean and Noodle S...</td>\n",
       "      <td>https://cooking.nytimes.com/recipes/1020205-as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://static01.nyt.com/images/2019/05/14/din...</td>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>Samin Nosrat</td>\n",
       "      <td>Named for the city in southwestern Iran, salad...</td>\n",
       "      <td>Salad-e Shirazi (Persian Cucumber, Tomato and ...</td>\n",
       "      <td>https://cooking.nytimes.com/recipes/1020212-sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://static01.nyt.com/images/2019/05/15/din...</td>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>Samin Nosrat</td>\n",
       "      <td>Fesenjoon hails from the verdant northern Iran...</td>\n",
       "      <td>Khoresh-e Fesenjoon (Persian Chicken Stew With...</td>\n",
       "      <td>https://cooking.nytimes.com/recipes/1020224-kh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://static01.nyt.com/images/2019/05/15/din...</td>\n",
       "      <td>2019-05-14</td>\n",
       "      <td>Samin Nosrat</td>\n",
       "      <td>Yogurt, both plain and with cucumbers, is ever...</td>\n",
       "      <td>Mast-o Khiar (Persian Cucumber and Herb Yogurt)</td>\n",
       "      <td>https://cooking.nytimes.com/recipes/1020213-ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://static01.nyt.com/images/2019/05/15/din...</td>\n",
       "      <td>2019-05-09</td>\n",
       "      <td>Rebekah Peppler</td>\n",
       "      <td>This simple recipe takes the 3-2-1 spritz form...</td>\n",
       "      <td>Amaro Spritz</td>\n",
       "      <td>https://cooking.nytimes.com/recipes/1020201-am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image_url   pub_date  \\\n",
       "0  https://static01.nyt.com/images/2019/05/15/din... 2019-05-14   \n",
       "1  https://static01.nyt.com/images/2019/05/14/din... 2019-05-14   \n",
       "2  https://static01.nyt.com/images/2019/05/15/din... 2019-05-14   \n",
       "3  https://static01.nyt.com/images/2019/05/15/din... 2019-05-14   \n",
       "4  https://static01.nyt.com/images/2019/05/15/din... 2019-05-09   \n",
       "\n",
       "     recipe_author                                 recipe_description  \\\n",
       "0     Samin Nosrat  Ash reshteh’s flavor is defined by two uniquel...   \n",
       "1     Samin Nosrat  Named for the city in southwestern Iran, salad...   \n",
       "2     Samin Nosrat  Fesenjoon hails from the verdant northern Iran...   \n",
       "3     Samin Nosrat  Yogurt, both plain and with cucumbers, is ever...   \n",
       "4  Rebekah Peppler  This simple recipe takes the 3-2-1 spritz form...   \n",
       "\n",
       "                                        recipe_title  \\\n",
       "0  Ash Reshteh (Persian Greens, Bean and Noodle S...   \n",
       "1  Salad-e Shirazi (Persian Cucumber, Tomato and ...   \n",
       "2  Khoresh-e Fesenjoon (Persian Chicken Stew With...   \n",
       "3    Mast-o Khiar (Persian Cucumber and Herb Yogurt)   \n",
       "4                                       Amaro Spritz   \n",
       "\n",
       "                                          recipe_url  \n",
       "0  https://cooking.nytimes.com/recipes/1020205-as...  \n",
       "1  https://cooking.nytimes.com/recipes/1020212-sa...  \n",
       "2  https://cooking.nytimes.com/recipes/1020224-kh...  \n",
       "3  https://cooking.nytimes.com/recipes/1020213-ma...  \n",
       "4  https://cooking.nytimes.com/recipes/1020201-am...  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to do all the cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_stem(word_stemmer, data_series):\n",
    "    \n",
    "    # removes numbers, punctuation, lowercases, and 2 special cases\n",
    "    alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "    punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "    extra_punc = lambda x: x.replace('’', '').replace('—', ' ')\n",
    "\n",
    "    initial_clean = data_series.map(alphanumeric).map(punc_lower).map(extra_punc)\n",
    "    \n",
    "    # word tokenize\n",
    "    tokenized = initial_clean.apply(word_tokenize)\n",
    "    \n",
    "    # stem using the passed in stemmer\n",
    "    stemmer = word_stemmer()\n",
    "    stemmed = tokenized.apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "    \n",
    "    # return the cleaned text back as a series of strings\n",
    "    return stemmed.apply(lambda x: ' '.join(x))\n",
    "\n",
    "def clean_text_lemmatize(word_lemmatizer, data_series):\n",
    "    \n",
    "    # removes numbers, punctuation, lowercases, and 2 special cases\n",
    "    alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "    punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "    extra_punc = lambda x: x.replace('’', '').replace('—', ' ')\n",
    "\n",
    "    initial_clean = data_series.map(alphanumeric).map(punc_lower).map(extra_punc)\n",
    "    \n",
    "    # single word tokenize\n",
    "    tokenized = initial_clean.apply(word_tokenize)\n",
    "    \n",
    "    # lemmatize using the passed in lemmatizer\n",
    "    lemmatizer = word_lemmatizer()\n",
    "    lemmatized = tokenized.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    \n",
    "    # return the cleaned text back as a series of strings\n",
    "    return lemmatized.apply(lambda x: ' '.join(x))\n",
    "\n",
    "def clean_text_spacy_lemmatizer(data_series):\n",
    "    #only works with spacy lemmatizer\n",
    "    \n",
    "    # removes numbers, punctuation, lowercases, and 2 special cases\n",
    "    alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "    punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "    extra_punc = lambda x: x.replace('’', '').replace('—', ' ')\n",
    "\n",
    "    initial_clean = data_series.map(alphanumeric).map(punc_lower).map(extra_punc)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmatizer = spacy.load('en')\n",
    "    lemmatized = lambda x: ' '.join([token.lemma_.strip() for token in lemmatizer(x) if token.lemma_ != '-PRON-'])\n",
    "\n",
    "    # return the cleaned text back as a series of strings\n",
    "    return initial_clean.apply(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/anaconda3/envs/metis/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.pipeline import FeatureUnion\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD # LSA\n",
    "from sklearn.decomposition import NMF # NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity # for LSA and NMF\n",
    "from gensim import corpora, models, similarities, matutils # LDA\n",
    "# logging for gensim (set to INFO)\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the formulas to do all the topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_topic_model(vectorizer, text, num_topics, len_topics=5):\n",
    "    # vectorize\n",
    "    docs_vect = vectorizer.fit_transform(text)\n",
    "    print(f\"document array shape is: {docs_vect.shape}\", '\\n')\n",
    "    \n",
    "    # model\n",
    "    model = TruncatedSVD(num_topics)\n",
    "    doc_topic = model.fit_transform(docs_vect)\n",
    "    \n",
    "    # print stats\n",
    "    print(f\"explained variance ratio: {model.explained_variance_ratio_}\")\n",
    "\n",
    "    ind=['component_'+str(num) for num in range(1,num_topics+1)]\n",
    "    # relating features to latent topics\n",
    "    topic_word = pd.DataFrame(model.components_.round(3),\n",
    "                 index = ind,\n",
    "                 columns = vectorizer.get_feature_names())\n",
    "#     print(topic_word)\n",
    "    display_topics(model, vectorizer.get_feature_names(), len_topics)\n",
    "    \n",
    "    # the Vt matrix shows us the documents we started with, and how each document is made up of the resulting topics\n",
    "#     Vt_mat = pd.DataFrame(doc_topic.round(3),\n",
    "#                  index = descr_df['recipe_title'],\n",
    "#                  columns = ind)\n",
    "#     print(Vt_mat.head())\n",
    "    \n",
    "    # prints the bag of words matrix as a dataframe\n",
    "    # print(pd.DataFrame(docs_vect.toarray(), index=descr_df['recipe_title'], columns=vectorizer.get_feature_names()).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_topic_model(vectorizer, text, num_topics, len_topics=5):\n",
    "    # vectorize\n",
    "    docs_vect = vectorizer.fit_transform(text)\n",
    "    print(f\"document array shape is: {docs_vect.shape}\", '\\n')\n",
    "    \n",
    "    # model\n",
    "    model = NMF(num_topics)\n",
    "    doc_topic = model.fit_transform(docs_vect)\n",
    "\n",
    "    ind=['component_'+str(num) for num in range(1,num_topics+1)]\n",
    "    # relating features to latent topics\n",
    "    topic_word = pd.DataFrame(model.components_.round(3),\n",
    "                 index = ind,\n",
    "                 columns = vectorizer.get_feature_names())\n",
    "#     print(topic_word)\n",
    "    display_topics(model, vectorizer.get_feature_names(), len_topics)\n",
    "    print(model.components_[:5])\n",
    "    \n",
    "    # the Vt matrix shows us the documents we started with, and how each document is made up of the resulting topics (how it maps into that space)\n",
    "#     Vt_mat = pd.DataFrame(doc_topic.round(3),\n",
    "#                  index = descr_df['recipe_title'],\n",
    "#                  columns = ind)\n",
    "#     print(Vt_mat.head())\n",
    "    \n",
    "    # prints the bag of words matrix as a dataframe\n",
    "    # print(pd.DataFrame(docs_vect.toarray(), index=descr_df['recipe_title'], columns=vectorizer.get_feature_names()).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gensim LSA model (called LSI)- try SKLearn version\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def lda_topic_model(vectorizer, text, num_topics, len_topics=5):\n",
    "    # vectorize\n",
    "    docs_vectorizer = vectorizer.fit(text)\n",
    "    \n",
    "    # Create the term-document matrix. Transpose it so the terms are the rows\n",
    "    doc_word = docs_vectorizer.transform(text).transpose()\n",
    "    \n",
    "    pd.DataFrame(doc_word.toarray(), docs_vectorizer.get_feature_names()).head()\n",
    "    print(f\"document array shape is: {doc_word.shape}\", '\\n')\n",
    "    \n",
    "    # Convert sparse scipy matrix of counts to a gensim corpus\n",
    "    corpus = matutils.Sparse2Corpus(doc_word)\n",
    "    \n",
    "    # map matrix rows to words. We need to save a mapping (dict) of row id to word (token) for later use by gensim\n",
    "    id2word = dict((v, k) for k, v in docs_vectorizer.vocabulary_.items())\n",
    "    \n",
    "    # Create lda model (equivalent to \"fit\" in sklearn)\n",
    "    # requires our corpus of word counts, mapping of row ids to words, and the number of topics\n",
    "    lda = models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, passes=5)\n",
    "    \n",
    "    # the 5 most important words for each of the topics we found\n",
    "    topics = lda.print_topics(len_topics)\n",
    "    for ix, topic in enumerate(topics):\n",
    "        print(f\"topic {ix}: {topic}\", \"\\n\")\n",
    "    \n",
    "    # to map our documents to the topic space we need to actually use the LdaModel transformer that we created above\n",
    "    # Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "    lda_corpus = lda[corpus]\n",
    "    # Store the documents' topic vectors in a list so we can take a peak\n",
    "    lda_docs = [doc for doc in lda_corpus]\n",
    "    \n",
    "    # take a look at the document vectors in the topic space, which are measures of the component of each document along each topic\n",
    "    # Check out the document vectors in the topic space for the first 5 documents\n",
    "    for ix, doc in enumerate(lda_docs[0:5]):\n",
    "        print(f'document {ix}: {doc}', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set stop words, adding new ones I come across, and stem if I used a stemmer\n",
    "def get_stop_words(word_stemmer=None):\n",
    "    stopwds = stopwords.words('english')\n",
    "    newStopWords = ['bake', 'kitchen', 'ingredient', 'dish', 'recipe', 'time', 'new', 'york',\n",
    "                   'make', 'use', 'like', 'one', 'add', 'made', 'list', 'step', 'flavor', 'also', 'stir',\n",
    "                   'without', 'invite', 'good', 'inch', 'serve']\n",
    "    stopwds.extend(newStopWords)\n",
    "\n",
    "    if word_stemmer:\n",
    "        # stem using the passed in stemmer\n",
    "        stemmer = word_stemmer()\n",
    "        return [stemmer.stem(word) for word in stopwds]\n",
    "    else:\n",
    "        return stopwds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you stem the text also stem the stop words\n",
    "cleaned_text = clean_text_stem(PorterStemmer, descr_df['recipe_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer\n",
    "lsa_topic_model(CountVectorizer(stop_words=get_stop_words(PorterStemmer), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "lsa_topic_model(TfidfVectorizer(stop_words=get_stop_words(PorterStemmer), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "           cleaned_text,\n",
    "           10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer\n",
    "nmf_topic_model(CountVectorizer(stop_words=get_stop_words(PorterStemmer), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Really good\n",
    "# TfidfVectorizer\n",
    "nmf_topic_model(TfidfVectorizer(stop_words=get_stop_words(PorterStemmer), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer\n",
    "lda_topic_model(CountVectorizer(stop_words=get_stop_words(PorterStemmer), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "lda_topic_model(TfidfVectorizer(stop_words=get_stop_words(PorterStemmer), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text_spacy_lemmatizer(descr_df['recipe_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer\n",
    "lsa_topic_model(CountVectorizer(stop_words=get_stop_words(), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "lsa_topic_model(TfidfVectorizer(stop_words=get_stop_words(), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "           cleaned_text,\n",
    "           10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer\n",
    "nmf_topic_model(CountVectorizer(stop_words=get_stop_words(), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "nmf_topic_model(TfidfVectorizer(stop_words=get_stop_words(), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer\n",
    "lda_topic_model(CountVectorizer(stop_words=get_stop_words(), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "lda_topic_model(TfidfVectorizer(stop_words=get_stop_words(), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text_lemmatize(WordNetLemmatizer, descr_df['recipe_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kept only because it has the printout I want\n",
    "# count vectorizer\n",
    "lsa_topic_model(CountVectorizer(stop_words=get_stop_words(), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "lsa_topic_model(TfidfVectorizer(stop_words=get_stop_words(), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "           cleaned_text,\n",
    "           10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer\n",
    "nmf_topic_model(CountVectorizer(stop_words=get_stop_words(), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "nmf_topic_model(TfidfVectorizer(stop_words=get_stop_words(), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer\n",
    "lda_topic_model(CountVectorizer(stop_words=get_stop_words(), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "lda_topic_model(TfidfVectorizer(stop_words=get_stop_words(), ngram_range=(1,2), min_df=3, max_df=.9), \n",
    "                cleaned_text, \n",
    "                10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(n_components=2, random_state=0,verbose=0)\n",
    "low_data = model.fit_transform(docs_vect.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = topics\n",
    "\n",
    "colors = cycle(['r','g','b','c','m','y','orange','k','aqua','yellow'])\n",
    "target_ids = range(len(target_names))\n",
    "plt.figure(dpi=150)\n",
    "for i, c, label in zip(target_ids, colors, target_names):\n",
    "    plt.scatter(low_data[target == i, 0], low_data[target == i, 1], c=c, label=label, s=15, alpha=1)\n",
    "plt.legend(fontsize=10, loc='upper left', frameon=True, facecolor='#FFFFFF', edgecolor='#333333')\n",
    "plt.xlim(-100,100);\n",
    "plt.title(\"Digit Clusters with TSNE\", fontsize=12)\n",
    "plt.ylabel(\"Junk TSNE Axis 2\", fontsize=12)\n",
    "plt.xlabel(\"Junk TSNE Axis 1\", fontsize=12);\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document array shape is: (6937, 17749) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# champion model is nmf with nltk lemmatizer\n",
    "\n",
    "#clean text\n",
    "cleaned_text = clean_text_lemmatize(WordNetLemmatizer, descr_df['recipe_description'])\n",
    "\n",
    "# vectorize\n",
    "vectorizer = TfidfVectorizer(stop_words=get_stop_words(), ngram_range=(1,2), min_df=3, max_df=.9)\n",
    "docs_vect = vectorizer.fit_transform(cleaned_text)\n",
    "print(f\"document array shape is: {docs_vect.shape}\", '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "salad, oil, olive, olive oil, green, dressing, garlic, lemon, pepper, herb\n",
      "\n",
      "Topic  1\n",
      "improvise, double, double boiler, boiler, diameter, chip, quince, fruit, leftover, smoker\n",
      "\n",
      "Topic  2\n",
      "pie, dough, butter, wa, sweet, fruit, sugar, flour, crust, drink\n",
      "\n",
      "Topic  3\n",
      "chicken, meat, sauce, fish, breast, pork, pan, cooking, roast, cook\n",
      "\n",
      "Topic  4\n",
      "bean, green, soup, green bean, white, black, white bean, black bean, pea, red\n",
      "\n",
      "Topic  5\n",
      "tomato, sauce, pasta, tomato sauce, summer, fresh, cheese, bread, eggplant, fresh tomato\n",
      "\n",
      "Topic  6\n",
      "potato, mashed, sweet potato, mashed potato, sweet, cheese, soup, gratin, sour cream, butter\n",
      "\n",
      "Topic  7\n",
      "cake, chocolate, frosting, layer, batter, pan, dessert, chocolate cake, moist, day\n",
      "\n",
      "Topic  8\n",
      "cream, ice, ice cream, chocolate, milk, coconut, dessert, vanilla, whipped, custard\n",
      "\n",
      "Topic  9\n",
      "rice, grain, vegetable, noodle, soup, cooked, brown rice, risotto, egg, meal\n",
      "Topic 0    0.015\n",
      "Topic 1    0.000\n",
      "Topic 2    0.021\n",
      "Topic 3    0.002\n",
      "Topic 4    0.019\n",
      "Topic 5    0.016\n",
      "Topic 6    0.016\n",
      "Topic 7    0.002\n",
      "Topic 8    0.000\n",
      "Topic 9    0.014\n",
      "Name: Spicy Chorizo and Red Lentil Soup with Kale, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = NMF(10) #number of topics\n",
    "doc_topic = model.fit_transform(docs_vect)\n",
    "\n",
    "# print topics\n",
    "display_topics(model, vectorizer.get_feature_names(), 10)\n",
    "\n",
    "# print weighting of each feature in the topics\n",
    "# print('printing model components', model.components_[:5])\n",
    "\n",
    "# the Vt matrix shows us the documents we started with, and how each document is made up of the resulting topics (how it maps into that space)\n",
    "ind=['Topic '+str(num) for num in range(0,10)]\n",
    "Vt_mat = pd.DataFrame(doc_topic.round(3),\n",
    "             index = descr_df['recipe_title'],\n",
    "             columns = ind)\n",
    "print(Vt_mat.loc['Spicy Chorizo and Red Lentil Soup with Kale'])\n",
    "\n",
    "# prints the bag of words matrix as a dataframe\n",
    "# print(pd.DataFrame(docs_vect.toarray(), index=descr_df['recipe_title'], columns=vectorizer.get_feature_names()).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_recipes(selected_recipe, num_recipes=3):\n",
    "    cos_sims=[]\n",
    "    labels=[]\n",
    "\n",
    "    for similar_recipe in descr_df[descr_df['recipe_title']!=selected_recipe]['recipe_title']:\n",
    "        cos_sims.append(cosine_similarity((Vt_mat.loc[selected_recipe], Vt_mat.loc[similar_recipe]))[0,1])\n",
    "        labels.append(similar_recipe)\n",
    "    df = pd.DataFrame(cos_sims, index=labels)\n",
    "\n",
    "    df = df.sort_values(by=0, ascending=False).head(num_recipes)\n",
    "    \n",
    "    for recipe in df.index:\n",
    "        details = descr_df[descr_df['recipe_title']==recipe]\n",
    "        print(recipe)\n",
    "        print(str(details['recipe_url']))\n",
    "        print(str(details['recipe_description']))\n",
    "        try: \n",
    "            display(Image(url=str(details['image_url']), height=500,width=500))\n",
    "        except:\n",
    "            print(\"This recipe does not have an image!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risotto With Spring Carrots and Leeks\n",
      "5350    https://cooking.nytimes.com/recipes/1012504-ri...\n",
      "Name: recipe_url, dtype: object\n",
      "5350    You can get carrots and leeks year ‘round in t...\n",
      "Name: recipe_description, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"5350    https://static01.nyt.com/images/2009/04/24/hea...\n",
       "Name: image_url, dtype: object\" width=\"500\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rye and Cornmeal Muffins With Caraway\n",
      "4611    https://cooking.nytimes.com/recipes/1013495-ry...\n",
      "Name: recipe_url, dtype: object\n",
      "4611    I like to serve these savory muffins, whose fl...\n",
      "Name: recipe_description, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"4611    https://static01.nyt.com/images/2011/03/01/sci...\n",
       "Name: image_url, dtype: object\" width=\"500\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lettuce Soup With Cucumber Croutons\n",
      "5951    https://cooking.nytimes.com/recipes/11000-lett...\n",
      "Name: recipe_url, dtype: object\n",
      "5951    Soup is the most versatile of dishes. When it ...\n",
      "Name: recipe_description, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"5951    https://static01.nyt.com/images/2014/06/02/din...\n",
       "Name: image_url, dtype: object\" width=\"500\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_similar_recipes('Spicy Chorizo and Red Lentil Soup with Kale')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
